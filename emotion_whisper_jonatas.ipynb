{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Emotion Detection using `jonatasgrosman/whisper-emo`"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers librosa soundfile torchaudio numpy -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files
",
        "uploaded = files.upload()
",
        "
",
        "audio_file = next(iter(uploaded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import librosa
",
        "import soundfile as sf
",
        "
",
        "def preprocess_audio(input_path, output_path="whisper_prepped.wav", target_sr=16000):
",
        "    y, sr = librosa.load(input_path, sr=None, mono=True)
",
        "    y = librosa.util.normalize(y)
",
        "    y_resampled = librosa.resample(y, orig_sr=sr, target_sr=target_sr)
",
        "    sf.write(output_path, y_resampled, target_sr)
",
        "    return output_path
",
        "
",
        "prepped_audio = preprocess_audio(audio_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline
",
        "
",
        "pipe = pipeline("audio-classification", model="jonatasgrosman/whisper-emo")
",
        "results = pipe(prepped_audio)
",
        "
",
        "for r in results:
",
        "    print(f"{r['label']}: {r['score']:.3f}")"
      ]
    }
  ],
  "metadata": {
    "colab": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}